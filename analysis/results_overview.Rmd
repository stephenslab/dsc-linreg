---
title: "Comparison of linear regression methods in the four scenarios from Zou & Hastie (2005)"
author: "Peter Carbonetto, Gao Wang and Matthew Stephens"
date: April 10, 2019
site: workflowr::wflow_site
output: workflowr::wflow_html
---

In this short analysis, we compare the prediction accuracy of several
linear regression in the four simulation examples of [Zou & Hastie
(2005)][zou-hastie-2005]. We also include two additional scenarios,
similar to Examples 1 and 2 from Zou & Hastie (2005): a "null"
scenario, in which the predictors have no effect on the outcome; and a
"one effect" scenario, in which only one of the predictors affects the
outcome.

The six methods compared are: (1) ridge regression; (2) the Lasso; (3)
the Elastic Net; (4) "Sum of Single Effects" (SuSiE) regression,
described [here][susie]; (5) variational inference for Bayesian
variable selection, or "varbvs", described [here][varbvs]; and (6)
"varbvsmix", an elaboration of varbvs that replaces the single normal
prior with a mixture-of-normals.

```{r knitr, echo=FALSE}
knitr::opts_chunk$set(comment = "#",results = "hold",collapse = TRUE,
                      fig.align = "center")
```

Load packages
-------------

Load a few packages and custom functions used in the analysis below.

```{r load-pkgs, warning=FALSE, message=FALSE}
library(dscrutils)
library(ggplot2)
library(cowplot)
source("../code/plots.R")
```

Import DSC results
------------------

Here we use function "dscquery" from the dscrutils package to extract
the DSC results we are interested in---the mean squared error in the
predictions from each method and in each simulation scenario. 

```{r import-dsc-results}
methods <- c("ridge","lasso","elastic_net","susie","varbvs","varbvsmix")
dsc <- dscquery("../dsc/linreg",
                c("simulate","fit","mse.err","simulate.scenario"),
				verbose = FALSE)
dsc <- transform(dsc,
                 simulate          = factor(simulate),
				 simulate.scenario = factor(simulate.scenario),
                 fit               = factor(fit,methods))
names(dsc)[1] <- "seed"
nrow(dsc)
```

After this call, the "dsc" data frame should contain results for 7,200
pipelines---6 methods times 6 scenarios times 200 data sets simulated
in each scenario.

```{r check-dsc-results-1}
nrow(dsc)
```

After these steps, the "dsc" data frame should have five columns:
"seed", the seed used to simulate the data; "simulate", the data
simulation module used; "simulate.scenario", the particular scenario
used in the "zh" simulation module; "fit", the linear regression
method used; and "mse.err", the mean squared error in the test set
predictions.

```{r check-dsc-results-2}
head(dsc)
```

Note that you will need to run the DSC before querying the results;
see [here](usage_instructions.html) for instructions on running the
DSC. If you did not run the DSC to generate these results, you can
replace the dscquery call above by this line to load the pre-extracted
results stored in a CSV file:

```{r import-results-from-csv, eval=FALSE}
dsc <- read.csv("../output/linreg_mse.csv")
```

This is how the CSV file was created:

```{r write-dsc-results}
write.csv(dsc,"../output/linreg_mse.csv",row.names = FALSE,quote = FALSE)
```

Summarize and discuss simulation results
----------------------------------------

Compute the mean squared error (MSE) in the predictions relative to
ridge regression, so that larger numbers mean greater error relative
to predictions from ridge regressions. 

```{r compute-relative-mse}
rmse <- compute.relative.mse(dsc)
dsc  <- cbind(dsc,rmse)
```

The boxplots below summarize the prediction errors in each of the
simulations. A relative MSE less than 1 indicates an improvement in
accuracy over ridge regression, whereas a relative MSE greater than 1
indicates a decrease in accuracy compared to ridge regression. Ridge
regression will always have a relative MSE of 1, so the results for
ridge regression are not shown.

```{r create-boxplots, fig.height=8, fig.width=7.5}
p1 <- rmse.boxplot(subset(dsc,simulate == "null_effects"),"null")
p2 <- rmse.boxplot(subset(dsc,simulate == "one_effect"),"one effect")
p3 <- rmse.boxplot(subset(dsc,simulate.scenario == 1),"scenario 1")
p4 <- rmse.boxplot(subset(dsc,simulate.scenario == 2),"scenario 2")
p5 <- rmse.boxplot(subset(dsc,simulate.scenario == 3),"scenario 3")
p6 <- rmse.boxplot(subset(dsc,simulate.scenario == 4),"scenario 4")
p  <- plot_grid(p1,p2,p3,p4,p5,p6)
print(p)
```

Here are a few initial impressions from these plots.

In most cases, the Elastic Net does at least as well, or better, than
the Lasso. This is what we would expect.

Ridge regression actually achieves good accuracy in all cases except
Scenario 4 and the "one effect" setting. Ridge regression is expected
to do less well in Scenario 4 because the majority of the true
coefficients are zero, so a sparse model would be favoured. Similarly,
a sparse model should better fit data simulated in the "one effect"
scenario.

In Scenario 4, where the predictors are correlated in a structured
way, and the effects are sparse, varbvs and varbvsmix perform
considerably better than the other methods.

The "varbvsmix" method yields competitive predictions in all
scenarios.

Session information
-------------------

```{r load-more-pkgs, include=FALSE}
library(MASS)
library(glmnet)
library(susieR)
library(varbvs)
```

The "Session information" button below gives the version of R and the
packages that were used to generate these results. This listing
includes the R packages that were also used to run the DSC.

[github-repo]:     https://github.com/stephenslab/dsc-linreg
[susie]:           https://doi.org/10.1101/501114
[varbvs]:          https://projecteuclid.org/euclid.ba/1339616726 
[zou-hastie-2005]: https://doi.org/10.1111/j.1467-9868.2005.00503.x
